# -*- coding: utf-8 -*-
"""Pea plant disease prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umv1YtMKj-cE60G_5OM9Ff24GUn-WWFn
"""

#DATA COLLECTION

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'pea-plant-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4031751%2F7012225%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240323%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240323T043138Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8e851e46d92c3c9fdc5b44bf225d56514db45b3314aaf6ccfe3d55a8bc67aef02df3b78931a0d538446f583e15141c7a26b3a8f7333111d44007f03301be84bffca5318141bc4ee4f9f4cc5129b00c04147ef7d66dc54b1b1c10d242ae89b51b6c9deb0e47f590aad4db324394ef37022c7b26cc73cf420ddf1cbfce9f796de02d9624255ecf6660a6ce65b531cb57725b3c511a84c4e81587b9950eb1ba147be8d6c1367d9131d7c92374b3836d934284097d2426fe1e9acef9abd091556fbbc78c5ddc1ee6e4dd5696b3448bc0576c48d78929c877d6a8b1ce5ee73806f76e2e724204655669d3f948c64ec96d96579a8ba5be3fdb7151a6e060686c520811,yolo-v5/tflite/tflite-tflite-model/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F1199%2F1418%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240323%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240323T043138Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1bfa5ac4d7508460f3e56a5cfa96885479d29c33c96610e612b88aec7699c498a8d3cb9347cc057497910ac6e526900ce568925ed1dc9b5bfcb33a3b1135dbc610871cd3dc744f5af522478fc012a81add8fe5a3cf36723f3c0d4b667d32c6b1377e69b6e9588a5cf6668e4fc67ac59eb4df39b0c7e7e3415d694730e8297faba5f329d5b482e8199c8c0f9e31b91821615c9180fd0d56f10e1b8cb711b9efbde43afb2f6c74aac8a384dcc6c8c1bd88594824d9680c63944020426a11fbf8bd59d195b3cc188a5c7d955063871d2c9a34ee3950bb081e766bc8c48717e1953d59c3c2549d7445628202d9db1831376f41db11cc04db1f4d6f4a5c22a51da851,bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F1899%2F2622%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240323%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240323T043138Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9ba4e75d8ea9d28e81daef549c6a90acc8cfefe64db4deaec73fad24c56347fd4295f7b3eb1975bf7cd99b993f5bcfec96a5dff52998dbf1f5fe6c07fa9f78e3d817831a126394de54f4ad79753912a5949cae35f10f14fd522a91df8473505b52b28e5902fbcc10b05954d8ac4476c8e404de09dae4fb6a8ee7668e4a6822352bba9e5b92f47a3638dc744208e3f3df3174fdf45de6c86f6e98e26e93571c681aeec3243fc54b05bb349fbd3f2213de514d2ff94528279f2347ab6bad4d6951031105e9d21d8645e5182bd7c43426587da9dc4c5ec3a60fa9a0897c895fbc46e272c2774833d2e5b6e7edd626bb850a100916ce7ae76a8b45765064884e3d8b'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

#PACKAGES
import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

#constants
IMAGE_SIZE = 256
BATCH_SIZE = 32
CHANNELS   = 3
EPOCHS = 20
FILE_PATH  = "/kaggle/input/pea-plant-dataset/Pea Plant dataset"

#READ DATASET
dataset = tf.keras.preprocessing.image_dataset_from_directory(
            directory=FILE_PATH,
            shuffle=True,
            image_size=(IMAGE_SIZE,IMAGE_SIZE),
            batch_size = BATCH_SIZE,
)

class_names = dataset.class_names
class_names

for image_batch, label_batch in dataset.take(1):
    print(image_batch.shape)
    print(label_batch.numpy())

#DATASET CONTENTS
plt.figure(figsize=(20,10))
for image_batch, label_batch in dataset.take(1):
    for i in range(15):
        ax = plt.subplot(3,5,i+1)
        plt.imshow(image_batch[i].numpy().astype('uint8'))
        plt.title(class_names[label_batch[i]])
        plt.axis("off")

def get_dataset_partitions(ds,train_split=0.8,test_split=0.1,valid_split=0.1,shuffle=True,shuffle_size=10000):
    assert  train_split+test_split+valid_split==1
    ds_size = len(ds)
    if shuffle:
        ds.shuffle(shuffle_size,seed=12)
    train_size = int(len(ds)*train_split)
    valid_size = int(len(ds)*valid_split)


    train_ds = ds.take(train_size)
    valid_ds = ds.skip(train_size).take(valid_size)
    test_ds = ds.skip(train_size).skip(valid_size)

    return train_ds ,test_ds, valid_ds

train_ds ,test_ds, valid_ds = get_dataset_partitions(dataset)

# performance improving by both using CPU and GPU
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
valid_ds = valid_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

#Image Preprocessing : Rescaling and Resizing
rescale_and_resize=tf.keras.Sequential([
    layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE) ,# will resize to the IMAGE_SIZE if there's any issue
    layers.experimental.preprocessing.Rescaling(1./255)  # Normalization
])

#data augmentation

data_augmentation = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2)
])

#MODEL ANALYSIS
input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = len(class_names)


model = models.Sequential([
    rescale_and_resize,
    data_augmentation,

    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
])

model.build(input_shape=input_shape)

model.summary()

#MODEL COMPILATION
model.compile(
    optimizer="adam",
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=["accuracy"]
)

# Example output layer for 4-class classification using TensorFlow
from tensorflow.keras import layers

model.add(layers.Dense(4, activation='softmax'))

import numpy as np
for images_batch, labels_batch in test_ds.take(1):

    first_image = image_batch[0].numpy().astype('uint8')
    first_label = labels_batch[0].numpy()

    print("first image to predict")
    plt.imshow(first_image)
    print("actual label:",class_names[first_label])

    batch_prediction = model.predict(images_batch)
    print("predicted label:",class_names[np.argmax(batch_prediction[0])])

def predict_tomato_diseases(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)


    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

#FINAL OUTPUT
plt.figure(figsize=(15, 15))
for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = predict_tomato_diseases(model, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")